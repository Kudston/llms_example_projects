{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1HV3ffv0ygQW2rrUsHvc8TEO7BWkoxbWj",
      "authorship_tag": "ABX9TyMQSPcuaev/IwVvj516Eq7Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kudston/llms_example_projects/blob/main/A_small_transformer_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BUILDING A SMALL TRANSFORMER MODEL FROM SCRATCH**"
      ],
      "metadata": {
        "id": "lDOA0sukEFYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the packages"
      ],
      "metadata": {
        "id": "BjoOkVlBEQR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ.setdefault(\"KERAS_BACKEND\", \"jax\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SZMQGixeEnXE",
        "outputId": "521a6d8a-33de-4787-e8c6-7506ce0347c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jax'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZLq6pcHDoBj",
        "outputId": "d7b40e9e-9285-4ce6-af28-3b48dff84206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade keras\n",
        "!pip install -q --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the packages"
      ],
      "metadata": {
        "id": "wK75tlrpEUQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "-D07J7P5D5Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOWNLOAD DATA FOR USE\n"
      ],
      "metadata": {
        "id": "jgaqXnaDE_Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "!tar -xf \"aclImdb_v1.tar.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Cal-OiFFQ9",
        "outputId": "836f3c5f-bd43-4432-dc2e-e9895c127572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  38.0M      0  0:00:02  0:00:02 --:--:-- 38.0M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -r \"aclImdb/train/unsup\""
      ],
      "metadata": {
        "id": "hnHq94txFZDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb/train\", batch_size=32, validation_split=0.2, subset=\"training\", seed=42\n",
        ")\n",
        "val_dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb/train\", batch_size=32, validation_split=0.2, subset=\"validation\", seed=42\n",
        ")\n",
        "test_dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb/test\", batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wNajhafGkF5",
        "outputId": "706a8d3b-f32d-43c7-f988-86956c21f887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE THE TOKENIZER"
      ],
      "metadata": {
        "id": "wP5tdFu7F_yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##USING WORLD PIECE TOKENIZER\n",
        "vocabs = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "  train_dataset.map(lambda x,y: x),\n",
        "  vocabulary_size= 20_000,\n",
        "  lowercase=True,\n",
        "  strip_accents=True,\n",
        "  reserved_tokens=['[PAD]','[START]','[END]','[CLS]','[SEP]','[MASK]','[UNK]']\n",
        ")\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocabs,\n",
        "    sequence_length=512,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    oov_token='[UNK]'\n",
        ")"
      ],
      "metadata": {
        "id": "m84vQKLlFf1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE THE TOKEN PARKER"
      ],
      "metadata": {
        "id": "_fXCYDBAJrR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=512,\n",
        "    start_value=tokenizer.token_to_id('[START]'),\n",
        "    end_value=tokenizer.token_to_id('[END]'),\n",
        "    pad_value=tokenizer.token_to_id('[PAD]'),\n",
        ")\n",
        "\n",
        "def preprocess(x, y):\n",
        "  packed_inputs = packer(tokenizer(x))\n",
        "  return packed_inputs, y\n",
        "\n"
      ],
      "metadata": {
        "id": "cteQB3yDJpaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##PREPROCESS THE DATASETS\n",
        "pretrained_ds = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "chXld05QDsFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pretrained_ds.unbatch().take(1).get_single_element())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywOjwHTBH8-z",
        "outputId": "d3719bf8-56c7-461d-8dcb-fb2fa931ab01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
            "array([    1,   271,   232,   111,   375,   392,  1060,    21,   109,\n",
            "         114,   165,    46,   503,   276,   102,  1134,    21,    54,\n",
            "         485,    19,   185,   148,   119,   124,    46,   173,   114,\n",
            "         115,   743,  6155,  1883,    21,   108,   107,    19,   682,\n",
            "         124,   102,   223,   177,  1979,    21,   121,   142,   101,\n",
            "        1380,   104,  2051,  1360,   153,   196,   114,   115,   123,\n",
            "         573,   751,    21,   109,   114,   111,   507,     8,    54,\n",
            "         314,   109,   114,  1379,   127,   685,   118,    46,   114,\n",
            "        4540,   108,     9,  2854,     9,   102,   135,   107,   111,\n",
            "        2995,   141,   104,   167,    19,    54,   527,    14,    65,\n",
            "        5438,  3996,   107,   153,   102,  2526,   107,   444,   115,\n",
            "         167,    21,   119,   148,   790,   167,    46,  4896,   848,\n",
            "          19,   116,   109,   114,   139,  3026,   157,   582,    19,\n",
            "         120,   104,   874,   188,   167,   567,   115,  1837,   128,\n",
            "         101,   267,   154,    21,   155,   246,   107,   188,   167,\n",
            "        1451,    19,    54,  1082,    19,   128,   101,   226,    19,\n",
            "         101,   318,   114,   139,  2051,  4089,   153,   157, 11232,\n",
            "         118,   206,  8606,    21,    54,  5753,   104,   157,   285,\n",
            "          19,   303,    19,   102,  1135,   581,   114,  4414,   103,\n",
            "          46,   665,    19,   132,   530, 11391,   109,   114,    21,\n",
            "         109,   105,    46,   114,   115,    46,   147,   382,   103,\n",
            "         643,    21,   107,   194,   190,   119,   567,   141,  1356,\n",
            "          19,  1293,   227,   582,    19,   190,   119,   879,   108,\n",
            "         212,    19,   102,   457,   227,   206,    21,   271,   154,\n",
            "         119,  1394,  4292,  1252,   602,   109,   114,    19,   139,\n",
            "         125,  2033,   110,   119,   122, 11434,   331,   101,  1213,\n",
            "         104,   514,   102,   329,   109,  3650,   114,    21,   132,\n",
            "         804,    38,   145,   143,   107,   194,   162,   675,   104,\n",
            "         119,    38,    19,   102,   489,   227,   430,  1018,   104,\n",
            "         101,  7176,  3935,   103,   206,    21,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     2],\n",
            "      dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DESIGN A SMALL TRANSFORMER MODEL"
      ],
      "metadata": {
        "id": "SLkYUcikISoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=tf.int32)\n",
        "##Add a token position embedding\n",
        "outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=20000,\n",
        "    sequence_length=512,\n",
        "    embedding_dim=128,\n",
        ")(inputs)\n",
        "\n",
        "##Add a transformer layer\n",
        "outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=128,\n",
        "    num_heads=2,\n",
        "    dropout=0.2\n",
        ")(outputs)\n",
        "\n",
        "##Add another transformer layer\n",
        "outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=128,\n",
        "    num_heads=2,\n",
        "    dropout=0.2\n",
        ")(outputs)\n",
        "\n",
        "##Add a pooling layer\n",
        "outputs = keras.layers.Dense(2, activation='softmax')(outputs[:,0,:])\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "QxkNez4gHNnS",
        "outputId": "34ba8afe-d497-468a-e80f-cafad844488e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │       \u001b[38;5;34m2,625,536\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m99,584\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m99,584\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_1 (\u001b[38;5;33mGetItem\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,625,536</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">99,584</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">99,584</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,824,962\u001b[0m (10.78 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,824,962</span> (10.78 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,824,962\u001b[0m (10.78 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,824,962</span> (10.78 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(3e-4),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        ")"
      ],
      "metadata": {
        "id": "xTIUnBsPGF6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    pretrained_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKAxvLzHF0pz",
        "outputId": "16400390-ebbd-40d0-fdcc-7e6805bf29e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - loss: 0.3750 - sparse_categorical_accuracy: 0.9360 - val_loss: 0.4354 - val_sparse_categorical_accuracy: 0.8740\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 37ms/step - loss: 0.3641 - sparse_categorical_accuracy: 0.9471 - val_loss: 0.4574 - val_sparse_categorical_accuracy: 0.8534\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - loss: 0.3681 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.4349 - val_sparse_categorical_accuracy: 0.8754\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - loss: 0.3586 - sparse_categorical_accuracy: 0.9542 - val_loss: 0.4417 - val_sparse_categorical_accuracy: 0.8686\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - loss: 0.3596 - sparse_categorical_accuracy: 0.9522 - val_loss: 0.4496 - val_sparse_categorical_accuracy: 0.8610\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - loss: 0.3579 - sparse_categorical_accuracy: 0.9548 - val_loss: 0.4403 - val_sparse_categorical_accuracy: 0.8714\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - loss: 0.3563 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.4451 - val_sparse_categorical_accuracy: 0.8674\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - loss: 0.3484 - sparse_categorical_accuracy: 0.9645 - val_loss: 0.4345 - val_sparse_categorical_accuracy: 0.8760\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - loss: 0.3446 - sparse_categorical_accuracy: 0.9679 - val_loss: 0.4357 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - loss: 0.3426 - sparse_categorical_accuracy: 0.9703 - val_loss: 0.4374 - val_sparse_categorical_accuracy: 0.8722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79ceba797790>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-oLB3dqGAO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}